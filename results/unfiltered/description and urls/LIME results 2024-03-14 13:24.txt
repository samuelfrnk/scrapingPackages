LIME search results:

URL: https://pypi.org/project/text-explainability/
Description: 
# LIME explanation (local feature importance) LIME (). explain (sample, model). scores # List of local rules, extracted from tree LocalTree (). explain (sample, model). rules. Global explanation: explain the whole dataset (e.g. train set, test set), and what they look like for the ground-truth or predicted labels.

URL: https://pypi.org/project/lime/
Description: 
lime 0.2.0.1 pip install lime Copy PIP instructions. Latest version. Released: Jun 26, 2020 Local Interpretable Model-Agnostic Explanations for machine learning classifiers. Navigation. Project description ; Release history ; Download files ; Project links. Homepage Statistics. GitHub statistics: ...

URL: https://pypi.org/project/dalex/
Description: 
add plot method and result attribute to LimeExplanation (use lime.explanation.Explanation.as_pyplot_figure() and lime.explanation.Explanation.as_list()) CeterisParibus.plot(variable_type='categorical') now has horizontal barplots - horizontal_spacing=None by default (varies on variable_type). Also, once again added the "dot" for observation value.

URL: https://pypi.org/project/keras-explain/
Description: 
exp_pos - explanation with marked features which contribute to the classification in a target class. exp_neg - explanation with marked features which contribute against the classification in a target class. LIME from keras_explain.lime_ribeiro import Lime explainer = Lime(model) exp_pos, exp_neg = explainer.explain(image, target_class) Parameters:

URL: https://pypi.org/project/scikit-explain/
Description: 
For LIME, scikit-explain uses the code from the Faster-LIME method. scikit-explain can create the summary and dependence plots from the shap python package, but is adapted for multiple features and an easier user interface. It is also possible to plot attributions for a single example or summarized by model performance.

URL: https://pypi.org/project/eli5/
Description: 
ELI5 is a Python package which helps to debug machine learning classifiers and explain their predictions. It provides support for the following machine learning frameworks and packages: scikit-learn. Currently ELI5 allows to explain weights and predictions of scikit-learn linear classifiers and regressors, print decision trees as text or as SVG ...

URL: https://pypi.org/project/shapash/
Description: 
Use Lime to compute local explanation, Summarize-it with Shapash; Compile faster Lime and consistency of contributions; Use FastTreeSHAP or add contributions from another backend; Use Class Shapash Backend; Evaluating the quality of your explainability. Building confidence on explainability methods using Stability, Consistency and Compacity metrics

URL: https://pypi.org/project/lime-python/
Description: 
Lime Python. Python implementation of LIME - A lightweight messaging library. LIME allows you to build scalable, real-time messaging applications using a JSON-based open protocol.It's asynchronous and supports any persistent transport like TCP or Websockets.. You can send and receive any type of object into the wire as long it can be represented as JSON or text (plain or encoded with base64 ...

URL: https://pypi.org/project/lime-stability/
Description: 
An explanation can be considered reliable only if unambiguous. Guided by this notion, we developed a pair of complementary indices to evaluate LIME stability: Variables Stability Index (VSI) and Coefficients Stability Index (CSI). The method creates repeated LIME explanations for the same data point to be explained.

URL: https://pypi.org/project/shap/
Description: 
Project description. SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions (see papers for details and citations).

URL: https://pypi.org/project/ferret-xai/
Description: 
Batched Inference for internal methods's approximation steps (e.g., LIME or SHAP) (v0.4.0) ⚙️ Simplified Task API to support NLI, Zero-Shot Text Classification, Language Modeling . ⚙️ Multi-sample explanation generation and evaluation; ⚙️ Support to explainers for seq2seq and autoregressive generation through inseq.

URL: https://pypi.org/project/omnixai/
Description: 
OmniXAI includes a rich family of explanation methods integrated in a unified interface, which supports multiple data types (tabular data, images, texts, time-series), multiple types of ML models (traditional ML in Scikit-learn and deep learning models in PyTorch/TensorFlow), and a range of diverse explaination methods including "model-specific ...

URL: https://pypi.org/project/pyxai/
Description: 
PyXAI (Python eXplainable AI) is a Python library (version 3.6 or later) allowing to bring formal explanations suited to (regression or classification) tree-based ML models (Decision Trees, Random Forests, Boosted Trees, ...). PyXAI generates explanations that are post-hoc and local. In contrast to many popular approaches to XAI (SHAP, LIME ...

URL: https://pypi.org/project/pytolemaic/
Description: 
Lime explanation: Provides Lime explanation for sample of interest. How to use: Get started by calling help() function (Recommended!): from pytolemaic import help supported_keys = help() # or help(key='basic usage') Example for performing all available analysis with PyTrust: from pytolemaic import PyTrust pytrust = PyTrust( model=estimator ...

URL: https://pypi.org/project/Metabokiller/
Description: 
LIME The biochemical property-focused Metabokiller, by the virtue of its construction, offers interpretability by implementing Local Interpretable Model-agnostic Explanations (LIME). An algorithm that provides interpretability with respect to carcinogen-specific biochemical properties for each SMILES provided. To activate interpretability using ...

URL: https://pypi.org/project/lemon-explainer/
Description: 
LEMON is a technique to explain why predictions of machine learning models are made. It does so by providing feature contribution: a score for each feature that indicates how much it contributed to the final prediction. More precisely, it shows the sensitivity of the feature: a small change in an important feature's value results in a relatively large change in prediction.

URL: https://pypi.org/project/lime-stable/
Description: 
Project description. This library provides a set of tools to fit lines in astronomical spectra. Its design aims for a user-friendly workflow for both single lines and large data sets. The library provides tools for masking, detecting, profile fitting and storing the results. . The output measurements are focused on the gas chemical and ...

URL: https://pypi.org/project/raiwidgets/
Description: 
The explanation functions of Interpret-Community accept both models and pipelines as input as long as the model or pipeline implements a predict or predict_proba function that conforms to the Scikit convention. If not compatible, you can wrap your model's prediction function into a wrapper function that transforms the output into the format ...

URL: https://pypi.org/project/bem/
Description: 
LIME explanations see their github # Explain the RF predictions # of the exoplanets from the test set bem.plot_LIME_predictions ( regr, dataset, train_test_sets ) # LIME explanation for your planet # in this case GJ 357 b bem.plot_LIME_predictions ( regr, dataset, train_test_sets, my_pred_planet = my_pred_planet, my_true_radius = 1 .166 )

URL: https://pypi.org/project/slise/
Description: 
To do this we replace the ground truth response vector with the predictions from the complex model. Furthermore, we force the model to fit a selected item (making the explanation local). This gives us a local approximation of the complex model with a simpler linear model (this is similar to, e.g., LIME and SHAP). In contrast to other methods ...

